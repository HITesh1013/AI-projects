# Bike Sharing Demand - Kaggle Competition
## Problem Overview
Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.

The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C. 

Task in this probelm is to predict the number of bike rentals for a day according to features provided in dataset. Information in features are as follows.

## Data Available
Train data which consists of total of 10886 data samples with 12 features
Test data consists of total of 6493 data samples with 10 features

## Features
1. Datetime: Deatetime feature indicates on which date the bike took on rent. It consists of an hourly date and timestamp.
2. Season:  Season feature indicates in which season bike was taken on rent. Season feature consists of 4 labels: </br> 
            spring - 1; Summer - 2; Fall - 3; Winter - 4.
3. Holiday: That particular date is a holiday or not: Holiday: 1; Not Holiday : 0
4. Weather: This feature indicates the weather of that particular season. It consists of 4 label </br>
   a. Clear, Few clouds, Partly cloudy, Partly cloudy </br>
   b: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist </br>
   c: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds </br>
   d: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
5. Temp:  Temperature of a particular day in degree Celcius
6. atemp: Temperature of air in degree Celcius
7. humidity: Relative humidity of that day
8. windspeed: windspeed of day
9. casual: Number of Non- Registered user
10. registered: Number of registered user
11. count: Predictive feature: have to count the number of rentals for that day

## Implementation approach
1. At the start, I have imported the required python3 libraries and then I'm reading provided data. i.e. train, test, sample submission.
2. While reading data I have converted the datetime column to datetime64 format so that I can use it as a feature instead of an index. This datetime column is now divided into 5 features year-month-day-hour-weekday. Applied this same on test data also.
3. Now preprocessing and data visualization methods are used to deal with the data.
4. From the data I have observed that registered and unregistered customers are forming linear relations with prediction label count. So to remove linearity I have deleted both the columns.
5. After that, I have used logarithmic transformation to manage the skewness of the count variable.
6. Model Building
   a. Random forest with bagging regressor: To train the model I have used a random forest regression algorithm along the bagging regressor with 15 estimators.
   b. XGBoost: In XG Boost I tried to used grid search for the decision tree algorithm. Here I have passed the dictionary of the learning rate, number of estimators, and depth of the tree.
8. After training, the model result is predicted and then using antilog to convert back into the main format.
9. The predicted result gives the Root Mean Squared Logarithmic Error (RMSLE) value of 0.42830 for random Forest and 0.39621 for XGBoost which is comparatively better than all others.

## Result with different models
1. XGBoost with grid search : 0.39621
2. XGBoost without grid search : 0.42375
3. Random forest with grid Search and bagging : 0.42830
4. Random forest without bagging : 0.43912
5. Decision tree : 0.49613
6. Linear Regression : 1.44315
